# day13 
## paper reading
- training LLMs with MXFP4 ( https://arxiv.org/abs/2502.20586 )

## Introduction  
- directly using `MXFP4` instead of `BF16` during training significantlyy degrades model quality.
- MXFP4 GEMM faster 2x than FP8 gregrade. 
- key insight: 
  - unbiased gradient estimate with `stochastic rounding`, more accurate. 
  - random Hadamard transform, threat outliters.
  - using 0.5x flops in mxfp4 thanl fp8. 
  - fp8 speedup > 1.3x , 
  - bp16 speedup 1.7x  during backpropagation.

![alt text](image.png)


- why using FP4 to training ?
    - pros: save memory, speed up computationğŸ‘Œ
    - cons: low precision, small range
- why MX ( microscaing ) FP4 ?
    - pros: larger range, better precision, hardware friendlyğŸ‘Œ
    - cons: more complex implementation
    - using `int8` be scale `s`, 8 bit / 32 = 0.25 bits per entry, so every number using 4.25bit

- tradition quantize method used fp32/fp16 been scale factor. 
- MXFP4: using 2^x been scale factor.


## algo 1
![alt text](image-1.png)

- algo 1 ä¸»è¦æ˜¯è¦æ‰¾åˆ°block è£¡é¢çš„ scale, æ‰¾çš„æ–¹å¼å¦‚ä¸Šæ–¹
1. å…ˆæ‰¾åˆ° block è£¡é¢çš„æœ€å¤§ value, åœ¨ç”¨ log2 æ‰¾åˆ° max value çš„ exponent. 
2. ä¸Šæ–¹æ‰¾åˆ°çš„ exponent - 2 (fp4 æœ€å¤§ exp == 2, å› ç‚ºfp4 exp åªç”¨ 2bit)
3. ç¬¬å››è¡ŒæœƒæŠŠ block è£¡é¢æ¯å€‹ value éƒ½ é™¤ä»¥ 2^shared_exp
## algo 2
![alt text](image-3.png)
- ç›®æ¨™ æœƒæ˜¯è¦è®“ quantize ä¹‹å¾Œçš„ value, åœ¨ fp4 è£¡é¢å¹³å‡åˆ†å¸ƒ
- dithering 
- è§£æ±º algo1 çš„ unbiased å•é¡Œ 
    - reduce scale. x * 3/4 ã€€( å› ç‚º fp4 æœ€å¤§ value æœ‰å¯èƒ½æœƒä»‹æ–¼ 6 ~ 8 ä¹‹é–“, é€™æ¨£å°±æœƒæœ‰æ©Ÿç‡ round åˆ° 8, 8 æ˜¯ overflow æ‰€ä»¥æœƒéœ€è¦æŠŠ valueç¸®å°æˆ 6/8 = 3/4) 
## algo 3
![alt text](image-4.png)
- ä½¿ç”¨ random hadamard transform å¯ä»¥æŠŠ outlier æŠ¹å¹³, ä¹‹å¾Œåœ¨ç”¨ é€†çŸ©é™£ è½‰å›ä¾†.
![alt text](image-2.png)
## Related Work  


## Method  
https://github.com/amazon-science/mxfp4-llm

- paper æ˜¯ç”¨ https://github.com/microsoft/microxcaling/tree/7bc41952de394f5cc5e782baf132e7c7542eb4e4 ç”¨ è»Ÿé«”æ¨¡æ“¬ mxfp4. å¦‚æœç”¨ ç¡¬é«”æ”¯æ´ mxfp4 æ‡‰è©²å¯ä»¥æ›´å¡Š
## Experiments and Results  
![](image-5.png)
- å¾çµæœä¾†çœ‹ å¯ä»¥çœ‹åˆ° mxfp4 åœ¨ bp16 çš„ loss å·²ç¶“å¾ˆæ¥è¿‘äº†
## Conclusion  
- ä¹‹å¾Œæœƒæƒ³è©¦çœ‹çœ‹ rtx5090, b200æ¨¡æ“¬ mxfp4 æ˜¯ä¸æ˜¯ æ¯” microxcaling é‚„è¦å¿«.